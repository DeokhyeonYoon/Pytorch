{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing Model Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Now that we have a model and data it’s time to train, validate and test our model by optimizing its parameters on our data.\n",
    " - Training a model is an iterative process; in each iteration the model makes a guess about the output, calculates the error in its guess (loss), collects the derivatives of the error with respect to its parameters (as we saw in the previous section), and optimizes these parameters using gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisite Code\n",
    "we load the code from the previous sections on Datasets & DataLoaders and Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dbsejrgus226/miniconda3/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/dbsejrgus226/miniconda3/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/dbsejrgus226/miniconda3/lib/python3.8/site-packages/torchvision/image.so: undefined symbol: _ZNK3c107SymBool10guard_boolEPKcl'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64)\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "Hyperparameters are adjustable parameters that let you control the model optimization process.Different hyperparameter values can impact model training and convergence rates\n",
    " - Number of Epochs : dataset를 반복하는 횟수\n",
    " - Batch size : parameter가 업데이트되기 전에 네트워크를 통해 전파되는 데이터 샘플 수\n",
    " - Learning Rate : 각 batch/epoch에서 model parameter를 업데이트 하는 정도\n",
    "    - 값이 작을수록 학습 속도가 느려지고 값이 크면 훈련 중에 예측할 수 없는 동작이 발생할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization Loop\n",
    "hyperparameter를 설정한 후 optimization loop를 통해 model을 trainning 하고 optimization 할 수 있음. 최적화 루프의 각 반복을 epoch라고 함\n",
    " - Train Loop : 훈련 데이터 세트를 반복하고 최적의 매개변수로 수렴하려고 시도\n",
    " - The Validation/Test Loop : test dataset를 반복하여 모델 성능이 향상되는지 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function\n",
    " - Loss Function는 획득된 결과와 목표값의 차이 정도를 측정하는 것으로 훈련 시 최소화하고자 함\n",
    " - Loss를 계산하기 위해 주어진 input 데이터 샘플을 사용하여 예측하고 이를 실제 데이터 레이블 값과 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the loss function\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer\n",
    "optimization은 각 training step 에서 모델 오류를 줄이기 위해 모델의 parameter를 조정하는 과정\n",
    " - Optimization algorithms은 위 process가 수행되는 방법을 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside the training loop, optimization happens in three steps:\n",
    " - ``optimizer.zero_grad()`` : model parameter의 gradient를 재설정, 이중 계산을 방지하기 위해 각 반복마다 명시적으로 0을 지정\n",
    " - ``loss.backward()`` : prediction loss를 backpropagate(역전파)함, Pytorch는 loss의 gradient를 w.r.t로 저장\n",
    " - ``optimizer.step()`` : gradient가 있으면, ``optimizer.step()``을 통해 backward pass에서 수집된 gradient로 parameter를 조정 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Implementation\n",
    " - ``train_loop``: 최적화 코드에 대한 loop를 정의\n",
    " - ``test_loop``: 테스트 데이터에 대해 모델의 성능 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    # Set the model to training mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize the loss function and optimizer, and pass it to train_loop and test_loop. Feel free to increase the number of epochs to track the model’s improving performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.311211  [   64/60000]\n",
      "loss: 2.304592  [ 6464/60000]\n",
      "loss: 2.286362  [12864/60000]\n",
      "loss: 2.279516  [19264/60000]\n",
      "loss: 2.259472  [25664/60000]\n",
      "loss: 2.236116  [32064/60000]\n",
      "loss: 2.240813  [38464/60000]\n",
      "loss: 2.209957  [44864/60000]\n",
      "loss: 2.209824  [51264/60000]\n",
      "loss: 2.181401  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 35.9%, Avg loss: 2.176876 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.185322  [   64/60000]\n",
      "loss: 2.177688  [ 6464/60000]\n",
      "loss: 2.125262  [12864/60000]\n",
      "loss: 2.143444  [19264/60000]\n",
      "loss: 2.084878  [25664/60000]\n",
      "loss: 2.037133  [32064/60000]\n",
      "loss: 2.061258  [38464/60000]\n",
      "loss: 1.984179  [44864/60000]\n",
      "loss: 1.999742  [51264/60000]\n",
      "loss: 1.933973  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 55.0%, Avg loss: 1.926742 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.951146  [   64/60000]\n",
      "loss: 1.923960  [ 6464/60000]\n",
      "loss: 1.814510  [12864/60000]\n",
      "loss: 1.863097  [19264/60000]\n",
      "loss: 1.738659  [25664/60000]\n",
      "loss: 1.696143  [32064/60000]\n",
      "loss: 1.719164  [38464/60000]\n",
      "loss: 1.612552  [44864/60000]\n",
      "loss: 1.647942  [51264/60000]\n",
      "loss: 1.550545  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.2%, Avg loss: 1.557785 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.618477  [   64/60000]\n",
      "loss: 1.579479  [ 6464/60000]\n",
      "loss: 1.435539  [12864/60000]\n",
      "loss: 1.509791  [19264/60000]\n",
      "loss: 1.380595  [25664/60000]\n",
      "loss: 1.381795  [32064/60000]\n",
      "loss: 1.389390  [38464/60000]\n",
      "loss: 1.306210  [44864/60000]\n",
      "loss: 1.347351  [51264/60000]\n",
      "loss: 1.254255  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 63.5%, Avg loss: 1.273816 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.350260  [   64/60000]\n",
      "loss: 1.326202  [ 6464/60000]\n",
      "loss: 1.164276  [12864/60000]\n",
      "loss: 1.270392  [19264/60000]\n",
      "loss: 1.143569  [25664/60000]\n",
      "loss: 1.172745  [32064/60000]\n",
      "loss: 1.184778  [38464/60000]\n",
      "loss: 1.114315  [44864/60000]\n",
      "loss: 1.159109  [51264/60000]\n",
      "loss: 1.081071  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 65.1%, Avg loss: 1.098448 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.170200  [   64/60000]\n",
      "loss: 1.166122  [ 6464/60000]\n",
      "loss: 0.987327  [12864/60000]\n",
      "loss: 1.124002  [19264/60000]\n",
      "loss: 0.999698  [25664/60000]\n",
      "loss: 1.032995  [32064/60000]\n",
      "loss: 1.060439  [38464/60000]\n",
      "loss: 0.993911  [44864/60000]\n",
      "loss: 1.039513  [51264/60000]\n",
      "loss: 0.974917  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 66.0%, Avg loss: 0.987552 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.046945  [   64/60000]\n",
      "loss: 1.063099  [ 6464/60000]\n",
      "loss: 0.868568  [12864/60000]\n",
      "loss: 1.028852  [19264/60000]\n",
      "loss: 0.910467  [25664/60000]\n",
      "loss: 0.935018  [32064/60000]\n",
      "loss: 0.980249  [38464/60000]\n",
      "loss: 0.916522  [44864/60000]\n",
      "loss: 0.958129  [51264/60000]\n",
      "loss: 0.904986  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 67.2%, Avg loss: 0.913056 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.957101  [   64/60000]\n",
      "loss: 0.992761  [ 6464/60000]\n",
      "loss: 0.784621  [12864/60000]\n",
      "loss: 0.962468  [19264/60000]\n",
      "loss: 0.851469  [25664/60000]\n",
      "loss: 0.863150  [32064/60000]\n",
      "loss: 0.924490  [38464/60000]\n",
      "loss: 0.865272  [44864/60000]\n",
      "loss: 0.900011  [51264/60000]\n",
      "loss: 0.855227  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 68.2%, Avg loss: 0.859983 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.888568  [   64/60000]\n",
      "loss: 0.940998  [ 6464/60000]\n",
      "loss: 0.723056  [12864/60000]\n",
      "loss: 0.913389  [19264/60000]\n",
      "loss: 0.809453  [25664/60000]\n",
      "loss: 0.809343  [32064/60000]\n",
      "loss: 0.882569  [38464/60000]\n",
      "loss: 0.830315  [44864/60000]\n",
      "loss: 0.856827  [51264/60000]\n",
      "loss: 0.817433  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 69.4%, Avg loss: 0.820200 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.834108  [   64/60000]\n",
      "loss: 0.900405  [ 6464/60000]\n",
      "loss: 0.676090  [12864/60000]\n",
      "loss: 0.875602  [19264/60000]\n",
      "loss: 0.777538  [25664/60000]\n",
      "loss: 0.768020  [32064/60000]\n",
      "loss: 0.848866  [38464/60000]\n",
      "loss: 0.805009  [44864/60000]\n",
      "loss: 0.823335  [51264/60000]\n",
      "loss: 0.787259  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.788816 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
